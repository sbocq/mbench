/*
* Copyright (C) 2013 Alcatel-Lucent.
*
* See the NOTICE file distributed with this work for additional
* information regarding copyright ownership.
* Licensed to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License. You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing,
* software distributed under the License is distributed on an
* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
* KIND, either express or implied. See the License for the
* specific language governing permissions and limitations
* under the License.
*/

package mbench.benchmark

import mbench.MBench.logout

/**
 * The main benchmark class.
 *
 * A benchmark specifies a sequence of input values `I` that must be fed to a function under test, the
 * results to report for each execution of a test and how to report these results. Once this is
 * specified, the same benchmark object can be reused to evaluate different tests in multiple configurations.
 *
 * The evaluation is kicked off by invoking one of the `apply` methods. One kind of `apply` methods evaluate
 * tests that depend only on the input of the benchmark. The other kind of `apply` methods take
 * an additional configuration parameter, which may setup a different runtime for a test for each input
 * of the benchmark or contain additional static configuration parameters, which remain constant
 * throughout the execution of the benchmark (see [[mbench.benchmark.Config]]).
 *
 * To ensure that the evaluations of each tests in a benchmark are independent from each other, each
 * execution is forked in a fresh clone of the current JVM where the `apply` method is invoked. This
 * clone is started with the `-server` option and inherits the environment and the configuration options
 * of the current JVM (e.g. classpath, options, etc.). ''Note this method assumes that the JVM
 * from which benchmarks are launched is a server VM, else this method will fail''.
 *
 * The execution times of a test are measured for each input of the benchmark and then reported
 * in real-time in a table, where each row corresponds to one input of the benchmark, which is
 * stored in the first column. By default the table is written to a '.dat' file where column values
 * are separated by spaces like this:
 * {{{
 * #|             cycles |            time[s] |            cvar[%]
 *            5000000                 .006                 .000
 *           10000000                 .012                3.666
 *           15000000                 .019                2.823
 * }}}
 *
 * Tables generated by benchmarks feature always at least these 3 columns:
 *
 *   - `input`: the input of the test, whose label is specified when the benchmark is created.
 *   - `time`: the median time of a test in seconds, which is evaluated over several runs.
 *   - `cvar`: the coefficient of variation measured across the different runs.
 *
 * End users can add new columns (see [[mbench.benchmark.Column]]) to compute the
 * speedup or the throughput of a test using the `add` method. For example, a throughput
 * column might be added like this:
 *
 * {{{
 *   val cycles = Label[Int]("cycles")
 *
 *   val throughput =
 *     Column[Int, Double]("throughput", cycles.perSeconds)((cycles, time) => cycles / time)
 *
 *   // or:
 *
 *   val throughput = Column.input(cycles).throughput
 *
 *   // or simply :
 *
 *   val throughput = Column.throughput(cycles)
 *
 *   val benchmark = Benchmark("loops", input, cycles, warmups = 2, runs = 5).add(throughput)
 *
 * }}}
 *
 * Columns may also depend on static configuration parameters `C` passed
 * to a test. Examples of this can be found in the companion project `mbench-benchmarks`.
 *
 * Benchmark are created using the factory methods in the companion object of this class.
 *
 * @tparam I  The type of input fed by a benchmark to a test.
 * @tparam C  The type of static configuration required by the columns of this benchmark.
 * @tparam R  The type of result reported by the benchmark.
 *
 * @param name     The name of the benchmark.
 * @param is       The input sequence.
 * @param ilabel   The input label.
 * @param columns  Columns defined by the end user in addition to `input`, `time` and `cvar`.
 * @param reporter A reporter used to report the benchmark (default is a `.dat` file).
 * @param warmups  Number of runs used to warmup the JVM.
 * @param runs     Number of runs used to compute the median and the coefficient of variation of the execution timings.
 * @param quiet    If set to true, any message sent on the standard output by the cloned JVM will be discarded.
 */
case class Benchmark[I, -C, R](
    name: String,
    is: Seq[I],
    ilabel: Label[I],
    columns: Vector[Column[I, C, _]],
    reporter: Reporter[R],
    warmups: Int,
    runs: Int,
    quiet: Boolean) {

  /**
   * Create a new benchmark with a different input sequence.
   *
   * @param is the new input sequence.
   * @return the new benchmark
   */
  def input(is: Seq[I]): Benchmark[I, C, R] =
    copy(is = is)

  /**
   * Create a new benchmark with a different reporter.
   *
   * @param reporter the new reporter.
   * @return the new benchmark.
   */
  def reporter[R](reporter: Reporter[R]): Benchmark[I, C, R] =
    copy(reporter = reporter)

  /**
   * Create a new benchmark with an additional column.
   *
   * @param column the new column.
   * @return the new benchmark.
   */
  def add[A <: C](column: Column[I, A, _]): Benchmark[I, A, R] =
    copy(columns = columns :+ column)

  /**
   * Extend the configuration of a benchmark (see also `StaticConfig#extend`).
   */
  def extend[D]: Benchmark[I, (C, D), R] =
    copy(columns = columns.map(_.extend[D]))

  //
  // Simple tests
  //

  /**
   * Benchmark a test for each input of the benchmark.
   *
   * @param test a test that depends only on the input.
   *
   * @return the report of the benchmark, as established by its reporter.
   */
  def apply(test: InputTest[I])(implicit ev: Any <:< C): R =
    apply(Config.empty, test)

  /**
   * Benchmark a test for each input of a benchmark and a given configuration.
   *
   * @param config  the configuration.
   * @param test    the test that depends on the configuration.
   *
   * @return the report of the benchmark, as established by its reporter.
   */
  def apply[S, Ci <: C](config: Config[I, Ci, S], test: Test[S, Ci, I]): R =
    mbench.net.Clone.start(this, config, test, quiet)

  /**
   * Perform a dry run evaluation of a test for a given input and returns
   * the median of its execution times and their coefficient of variation
   * computed over multiple runs.
   *
   * @param runs   the number of runs to perform in the dry run.
   * @param i      the input to the dry run.
   * @param test    the test to benchmark.
   * @return a measure that contains the median of the execution times and
   *         their coefficient of variation.
   */
  def dryRun(runs: Int, i: I, test: InputTest[I])(implicit ev: Any <:< C): Measure =
    dryRun(runs, i, Config.empty, test)

  /**
   * Perform a dry run evaluation of a test for a given input and returns
   * the median of its execution times and their coefficient of variation
   * computed over multiple runs.
   *
   * @param runs   the number of runs to perform in the dry run.
   * @param i      the input to the dry run.
   * @param config a configuration for the test.
   * @return a measure that contains the median of the execution times and
   *         their coefficient of variation.
   */
  def dryRun[S, Ci <: C](runs: Int, i: I, config: Config[I, Ci, S], test: Test[S, Ci, I]): Measure = {
    val setup = config.setUp(i)
    val m = _eval("dry-run", runs, config, i, test)
    config.tearDown(setup)
    m
  }

  /**
   * Benchmark a test for each value in the sequence of input data and a given configuration
   * without forking a clone of this JVM.
   *
   * @param config  the test configuration.
   * @param test    the test to benchmark.
   *
   * @return the report of the benchmark, as established by its reporter.
   */
  def local[S, Ci <: C](config: Config[I, Ci, S], test: Test[S, Ci, I]): R = {

    // Very important to move these two statements before warmups for microbenchmarks!
    val (labels, rowF) = Column.toFunction(ilabel, columns)
    val report = reporter.open(name, test.name, config, labels)

    if (warmups > 0) {
      _eval("warmup", warmups, config, is.head, test)
    }
    is.foldLeft((EmptyState, report)) {
      case ((s0, r), i) =>
        val evaluation = _eval("benchmark", runs, config, i, test)
        val (s1, values) = rowF(s0, i, config.value, evaluation.time, evaluation.cvar)
        (s1, r.update(values))
    }._2.close()
  }

  private[this] def _eval[S, Ci <: C](phase: String, runs: Int, config: Config[I, Ci, S], i: I, test: Test[S, Ci, I]): Measure = {
    require(runs > 0, "Number of runs must be greater than 0 for test " + test.name)
    logout.println(phase + ":" + name + ":" + test.name + ":" + config + ":" + ilabel.format(i))
    val setup = config.setUp(i)
    val times =
      (1 to runs).foldLeft(Vector.empty[Double]) { (times, _) =>
        val (time, _) = Measure.time(test.run(setup, config.value, i))
        times :+ time
      }
    config.tearDown(setup)
    Measure(times)
  }

  //
  // Sequence tests
  //

  /**
   * Benchmark a test for each input of the benchmark.
   *
   * @param test the test to benchmark.
   *
   * @return the report of the benchmark, as established by its reporter.
   */
  def seq(test: InputTestSeq[I])(implicit ev: Any <:< C): Seq[R] =
    seq(Config.empty, test)

  /**
   * Perform a dry run evaluation of a test for a given input and returns
   * the median of its execution times and their coefficient of variation
   * computed over multiple runs.
   *
   * @param runs   the number of runs to perform in the dry run.
   * @param i      the input to the dry run.
   * @param test    the test to benchmark.
   * @return a measure that contains the median of the execution times and
   *         their coefficient of variation.
   */
  def dryRunSeq(runs: Int, i: I, test: InputTestSeq[I])(implicit ev: Any <:< C): Measure =
    dryRunSeq(runs, i, Config.empty, test)

  import TestSeq.{ State => SeqState }

  /**
   * Benchmark a test for each input of a benchmark and a given configuration.
   *
   * @param config  the configuration.
   * @param test    the test that depends on the configuration.
   *
   * @return the report of the benchmark, as established by its reporter.
   */
  def seq[S, Ci <: C](config: Config[I, Ci, S], test: TestSeq[S, Ci, I]): Seq[R] =
    mbench.net.Clone.start(this, config, test, quiet)

  /**
   * Benchmark a test for each value in the sequence of input data and a given configuration
   * without forking a clone of this JVM.
   *
   * @param config  the test configuration.
   * @param test    the test to benchmark.
   *
   * @return the report of the benchmark, as established by its reporter.
   */
  def local[S, Ci <: C](config: Config[I, Ci, S], test: TestSeq[S, Ci, I]): Seq[R] = {

    // Very important to move these statements before warmups for microbenchmarks!
    val (labels, rowF) = Column.toFunction(ilabel, columns)

    val setup = config.setUp(is.head)
    val (testNames, _) = _evalSeq("analyzing sequence", 1, setup, config, is.head, test)
    val rs0: Vector[(State, Report[R])] =
      testNames map (n => (EmptyState, reporter.open(name, n, config, labels)))

    if (warmups > 0)
      _evalSeq("warmup", warmups, setup, config, is.head, test)

    config.tearDown(setup)

    val rs = is.foldLeft(rs0) { (rs, i) =>
      benchmarkSeq(runs, rs, rowF, config, i, test)
    }

    rs map { case (_, r) => r.close() }
  }

  private[this] def benchmarkSeq[S, Ci <: C](runs: Int, rs: Vector[(State, Report[R])], rowF: Column.MFunction[I, C], config: Config[I, Ci, S], i: I, test: TestSeq[S, Ci, I]): Vector[(State, Report[R])] = {
    val setup = config.setUp(i)
    val (_, measures) = _evalSeq("benchmark", runs, setup, config, i, test)
    config.tearDown(setup)

    // Invoke the reporter of each test on new data
    (measures zip rs) map {
      case (measure, (s0, report)) =>
        val (s1, values) = rowF(s0, i, config.value, measure.time, measure.cvar)
        (s1, report.update(values))
    }
  }

  /**
   * Perform a dry run evaluation of a test for a given input and returns
   * the median of its execution times and their coefficient of variation
   * computed over multiple runs.
   *
   * @param runs   the number of runs to perform in the dry run.
   * @param i      the input to the dry run.
   * @param config a configuration for the test.
   * @return a measure that contains the median of the execution times and
   *         their coefficient of variation.
   */
  def dryRunSeq[S, Ci <: C](runs: Int, i: I, config: Config[I, Ci, S], test: TestSeq[S, Ci, I]): Measure = {
    val setup = config.setUp(i)
    val (_, ms) = _evalSeq("dry-run", runs, setup, config, i, test)
    config.tearDown(setup)
    Measure.sum(ms)
  }

  /**
   * Return test names and measurements for each test
   */
  private[this] def _evalSeq[S, Ci <: C](phase: String, runs: Int, setup: S, config: Config[I, Ci, _], i: I, test: TestSeq[S, Ci, I]): (Vector[String], Vector[Measure]) = {
    require(runs > 0, "Number of runs must be greater than 0 for benchmark " + name)
    logout.println(phase + ":" + name + ":" + config + ":" + ilabel.format(i))

    // 1) Collect names and results of evaluations (Long, Long) for each test in each run

    // Test results for each run
    // Vector[runNo][TestNo](Time)
    val ress0 = Vector[Vector[Double]]()

    val (names, ress) =
      (1 to runs).foldLeft((Vector.empty[String], ress0)) {
        case ((_, ess), _) =>
          val (SeqState(names, es), _) = test.run(setup, config.value, i).run(SeqState.empty)
          // Names should be the same in each run if end user doesn't use branches in TestSeq...
          (names, ess :+ es)
      }

    // 2) Get test results for each test.
    // Vector[testNo][runNo](Time)
    val tess = ress.transpose

    // 3) Compact test results to sequence of results Seq[Double]
    val mss = tess.map { ess =>
      ess.foldLeft(Vector.empty[Double]) {
        case (vs, time) =>
          vs :+ time
      }
    }.map(Measure.apply)

    (names, mss)
  }

  /**
   * Executes a benchmark that generates ideal times from a function of the input.
   *
   * @param time a function that computes the ideal time from the input.
   * @return the reporter's result.
   */
  def ideal(testName: String, time: I => Double)(ev: Any <:< C): R =
    ideal(testName, ev(None), time)

  /**
   * Executes a benchmark that generates ideal times from a function of the input
   * @param value a static configuration parameter required by the columns.
   * @param time  a function that computes the ideal time from the input.
   * @return the reporter's result.
   */
  def ideal(testName: String, config: C, time: I => Double): R = {
    val idealName = "ideal-" + testName
    val (labels, rowF) = Column.toFunction(ilabel, columns)
    val report = reporter.open(name, idealName, Config.empty, labels)
    is.foldLeft((EmptyState, report)) {
      case ((s0, r), i) =>
        val evaluation = Measure(time(i), 0)
        val (s1, values) = rowF(s0, i, config, evaluation.time, evaluation.cvar)
        (s1, r.update(values))
    }._2.close()
  }

}

/**
 * Factory methods for benchmarks.
 */
object Benchmark {
  import mbench.gnuplot.{ DatFile, DatFileReporter }

  /**
   * Create a benchmark that reports its result in a `.dat` file.
   *
   * @param name     The name of the benchmark.
   * @param is       The input sequence.
   * @param ilabel   The input label.
   * @param warmups  Number of runs used to warmup the JVM.
   * @param runs     Number of runs used to compute the median and the coefficient of variation of the execution timings.
   */
  def apply[I <: AnyVal](name: String, is: Seq[I], ilabel: Label[I], warmups: Int, runs: Int): Benchmark[I, Any, DatFile] =
    Benchmark(name, is, ilabel, Vector.empty, DatFileReporter, warmups, runs, false)

  /**
   * Create a benchmark that reports its result in a `.dat` file.
   *
   * @param name     The name of the benchmark.
   * @param is       The input sequence.
   * @param ilabel   The input label.
   * @param warmups  Number of runs used to warmup the JVM.
   * @param runs     Number of runs used to compute the median and the coefficient of variation of the execution timings.
   * @param quiet    If set to true, any message sent on the standard output by the cloned JVM will be discarded.
   */
  def apply[I <: AnyVal](name: String, is: Seq[I], ilabel: Label[I], warmups: Int, runs: Int, quiet: Boolean): Benchmark[I, Any, DatFile] =
    Benchmark(name, is, ilabel, Vector.empty, DatFileReporter, warmups, runs, quiet)

  /**
   * Create a benchmark.
   *
   * @param name     The name of the benchmark.
   * @param is       The input sequence.
   * @param ilabel   The input label.
   * @param reporter A reporter used to report the benchmark (default is a `.dat` file).
   * @param warmups  Number of runs used to warmup the JVM.
   * @param runs     Number of runs used to compute the median and the coefficient of variation of the execution timings.
   * @param quiet    If set to true, any message sent on the standard output by the cloned JVM will be discarded.
   */
  def apply[I <: AnyVal, R](name: String, is: Seq[I], ilabel: Label[I], reporter: Reporter[R], warmups: Int, runs: Int, quiet: Boolean = false): Benchmark[I, Any, R] =
    Benchmark(name, is, ilabel, Vector.empty, reporter, warmups, runs, quiet)

}

